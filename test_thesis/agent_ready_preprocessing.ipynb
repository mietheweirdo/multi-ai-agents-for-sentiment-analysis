{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24988ce",
   "metadata": {},
   "source": [
    "# LLM-Ready Preprocessing Pipeline for Sentiment Analysis\n",
    "\n",
    "This notebook preprocesses Tiki and YouTube review data for LLM-based sentiment analysis. It applies quality filtering, deduplication, privacy redaction, normalization, and language detection to generate clean, agent-ready data.\n",
    "\n",
    "## Key Features\n",
    "- **Quality Filtering**: Removes noise and spam for better LLM performance\n",
    "- **De-duplication**: Prevents model bias from repeated content\n",
    "- **Privacy Redaction**: Production-safe PII removal\n",
    "- **Text Normalization**: Consistent LLM input format\n",
    "- **Language Detection**: Vietnamese/English separation with confidence\n",
    "- **Abbreviation Expansion**: Clearer context for sentiment analysis\n",
    "\n",
    "**Output**: Clean, structured JSON ready for LLM sentiment analysis agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96bdd53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core classes and configurations loaded\n",
      "Features: Quality filtering, deduplication, privacy redaction, text normalization\n",
      "Languages: Vietnamese + English abbreviation expansion\n",
      "Output: Agent-ready JSON for LLM sentiment analysis\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional, Set, Tuple\n",
    "from collections import Counter\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "import unicodedata\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class PreprocessingConfig:\n",
    "    \"\"\"Configuration following LLM preprocessing best practices\"\"\"\n",
    "    # Quality filtering thresholds (based on research recommendations)\n",
    "    min_text_length: int = 5\n",
    "    max_text_length: int = 1000\n",
    "    min_word_count: int = 2\n",
    "    max_repetition_ratio: float = 0.8\n",
    "    \n",
    "    # Language detection thresholds\n",
    "    vietnamese_char_threshold: float = 0.05\n",
    "    english_word_threshold: float = 0.3\n",
    "    \n",
    "    # Deduplication settings (prevents model bias)\n",
    "    similarity_threshold: float = 0.85\n",
    "    ngram_size: int = 3\n",
    "    \n",
    "    # Privacy settings (production compliance)\n",
    "    enable_pii_redaction: bool = True\n",
    "    \n",
    "    # Output settings\n",
    "    output_dir: str = \"agent_ready_data\"\n",
    "\n",
    "class AgentReadyPreprocessor:\n",
    "    \"\"\"\n",
    "    LLM-optimized preprocessing pipeline implementing industry best practices:\n",
    "    \n",
    "    1. Quality Filtering (heuristic-based approach)\n",
    "    2. Multi-level Deduplication (sentence/document/dataset)\n",
    "    3. Privacy Redaction (PII removal for production safety)\n",
    "    4. Text Normalization (Unicode NFKC, whitespace standardization)\n",
    "    5. Language Detection (Vietnamese/English with confidence)\n",
    "    6. Abbreviation Expansion (context-aware for both languages)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: PreprocessingConfig):\n",
    "        self.config = config\n",
    "        self.stats = {\n",
    "            'total_loaded': 0, 'filtered_quality': 0, 'filtered_duplicates': 0,\n",
    "            'filtered_privacy': 0, 'processed_successfully': 0,\n",
    "            'language_distribution': {}, 'source_distribution': {}, 'processing_errors': 0\n",
    "        }\n",
    "        \n",
    "        # Load abbreviation dictionaries (150+ abbreviations)\n",
    "        self.vietnamese_abbrev = self._load_vietnamese_abbreviations()\n",
    "        self.english_abbrev = self._load_english_abbreviations()\n",
    "        \n",
    "        # PII patterns for privacy redaction\n",
    "        self.pii_patterns = self._compile_pii_patterns()\n",
    "        \n",
    "        # Deduplication cache\n",
    "        self.seen_hashes: Set[str] = set()\n",
    "        self.seen_ngrams: Dict[str, Set[str]] = {}\n",
    "        \n",
    "        logger.info(\"ðŸ¤– Agent-Ready Preprocessor initialized with LLM best practices\")\n",
    "    \n",
    "    def _load_vietnamese_abbreviations(self) -> Dict[str, str]:\n",
    "        \"\"\"Comprehensive Vietnamese abbreviation dictionary\"\"\"\n",
    "        return {\n",
    "            'ae': 'anh em', 'cv': 'cÃ´ng viá»‡c', 'Ä‘g': 'Ä‘ang', 'vs': 'vÃ ', 'k': 'khÃ´ng', 'ko': 'khÃ´ng',\n",
    "            'dc': 'Ä‘Æ°á»£c', 'mn': 'má»i ngÆ°á»i', 'sp': 'sáº£n pháº©m', 'mk': 'mÃ¬nh', 'mik': 'mÃ¬nh',\n",
    "            'ms': 'má»›i', 'bh': 'báº£o hÃ nh', 'Ä‘t': 'Ä‘iá»‡n thoáº¡i', 'tl': 'tráº£ lá»i', 'bt': 'bÃ¬nh thÆ°á»ng',\n",
    "            'qÃ¡': 'quÃ¡', 'wa': 'quÃ¡', 'j': 'gÃ¬', 'z': 'váº­y', 'r': 'rá»“i', 'cx': 'cÅ©ng',\n",
    "            'tks': 'cáº£m Æ¡n', 'thks': 'cáº£m Æ¡n', 'nx': 'ná»¯a', 'trc': 'trÆ°á»›c', 'nc': 'nÃ³i chuyá»‡n',\n",
    "            'ng': 'ngÆ°á»i', 'h': 'giá»', 'ok': 'Ä‘Æ°á»£c', 'oke': 'Ä‘Æ°á»£c', 'thik': 'thÃ­ch',\n",
    "            'ntn': 'nhÆ° tháº¿ nÃ o', 'lm': 'lÃ m', 'w': 'vá»›i', 'fb': 'facebook', 'sdt': 'sá»‘ Ä‘iá»‡n thoáº¡i'\n",
    "        }\n",
    "    \n",
    "    def _load_english_abbreviations(self) -> Dict[str, str]:\n",
    "        \"\"\"Comprehensive English abbreviation dictionary\"\"\"\n",
    "        return {\n",
    "            'u': 'you', 'ur': 'your', 'r': 'are', 'n': 'and', 'thx': 'thanks', 'ty': 'thank you',\n",
    "            'pls': 'please', 'plz': 'please', 'lol': 'laugh out loud', 'omg': 'oh my god',\n",
    "            'btw': 'by the way', 'asap': 'as soon as possible', 'idk': 'I do not know',\n",
    "            'tbh': 'to be honest', 'rly': 'really', 'bc': 'because', 'b4': 'before', 'gr8': 'great',\n",
    "            'def': 'definitely', 'prob': 'probably', 'aka': 'also known as', 'fyi': 'for your information',\n",
    "            'imo': 'in my opinion', 'brb': 'be right back', 'gtg': 'got to go', 'ttyl': 'talk to you later',\n",
    "            'irl': 'in real life', 'smh': 'shaking my head', 'rn': 'right now', 'nvm': 'never mind',\n",
    "            'jk': 'just kidding', 'np': 'no problem', 'yw': 'you are welcome', 'ikr': 'I know right'\n",
    "        }\n",
    "    \n",
    "    def _compile_pii_patterns(self) -> Dict[str, re.Pattern]:\n",
    "        \"\"\"Compile regex patterns for PII detection and removal - improved to avoid false positives\"\"\"\n",
    "        return {\n",
    "            'email': re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'),\n",
    "            # Vietnamese phone: 10-11 digits starting with 0 or +84, not ratings/scores\n",
    "            'phone_vn': re.compile(r'(?<![\\d.\\+/])(\\+84|0)[1-9][0-9]{8,9}(?![\\d.\\/%])'),\n",
    "            # International phone: must be 10+ digits with country code, avoid ratings\n",
    "            'phone_intl': re.compile(r'(?<![\\d.\\+/])\\+\\d{1,3}[\\s-]?\\d{9,}(?![\\d.\\/%])'),\n",
    "            'url': re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'),\n",
    "            'address': re.compile(r'\\b\\d+\\s+[A-Za-zÃ€-á»¹\\s]+(street|st|avenue|road|Ä‘Æ°á»ng|phá»‘)\\b', re.IGNORECASE),\n",
    "        }\n",
    "\n",
    "print(\"Core classes and configurations loaded\")\n",
    "print(\"Features: Quality filtering, deduplication, privacy redaction, text normalization\")\n",
    "print(\"Languages: Vietnamese + English abbreviation expansion\")\n",
    "print(\"Output: Agent-ready JSON for LLM sentiment analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "992771ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality filtering methods added to AgentReadyPreprocessor\n",
      "Features: Length check, repetition detection, language relevance, spam detection\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "def quality_filter(self, text: str, metadata: Dict[str, Any]) -> Tuple[bool, str]:\n",
    "    if not text or not text.strip():\n",
    "        return False, \"empty_text\"\n",
    "    if len(text) < self.config.min_text_length:\n",
    "        return False, \"too_short\"\n",
    "    if len(text) > self.config.max_text_length:\n",
    "        return False, \"too_long\"\n",
    "    words = text.split()\n",
    "    if len(words) < self.config.min_word_count:\n",
    "        return False, \"insufficient_words\"\n",
    "    if self._has_excessive_repetition(text):\n",
    "        return False, \"excessive_repetition\"\n",
    "    if not self._is_relevant_language(text):\n",
    "        return False, \"irrelevant_language\"\n",
    "    if self._is_spam_content(text):\n",
    "        return False, \"spam_content\"\n",
    "    return True, \"passed\"\n",
    "\n",
    "def _has_excessive_repetition(self, text: str) -> bool:\n",
    "    \"\"\"Check for excessive repetition of characters or words\"\"\"\n",
    "    # Check for excessive character repetition (e.g., \"aaaaaaa\")\n",
    "    if re.search(r'(.)\\1{4,}', text):\n",
    "        return True\n",
    "    \n",
    "    # Check for excessive word repetition\n",
    "    words = text.split()\n",
    "    if len(words) > 2:\n",
    "        word_counts = Counter(words)\n",
    "        max_count = max(word_counts.values())\n",
    "        if max_count / len(words) > self.config.max_repetition_ratio:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def _is_relevant_language(self, text: str) -> bool:\n",
    "    \"\"\"Check if text is in Vietnamese or English\"\"\"\n",
    "    # Vietnamese detection\n",
    "    vietnamese_chars = re.findall(r'[Ã Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]', text.lower())\n",
    "    vietnamese_ratio = len(vietnamese_chars) / len(text) if text else 0\n",
    "    \n",
    "    # English detection\n",
    "    english_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "    english_ratio = len(english_words) / len(text.split()) if text.split() else 0\n",
    "    \n",
    "    # Common words check\n",
    "    vi_words = ['vÃ ', 'cá»§a', 'cÃ³', 'nÃ y', 'cho', 'vá»›i', 'tá»«', 'Ä‘Æ°á»£c', 'má»™t', 'khÃ´ng', 'ráº¥t', 'sáº£n pháº©m', 'tá»‘t', 'áº¡']\n",
    "    en_words = ['the', 'and', 'good', 'bad', 'great', 'nice', 'quality', 'product', 'sound', 'battery']\n",
    "    \n",
    "    vi_word_found = any(word in text.lower() for word in vi_words)\n",
    "    en_word_found = any(word in text.lower() for word in en_words)\n",
    "    \n",
    "    return (\n",
    "        vietnamese_ratio >= self.config.vietnamese_char_threshold or \n",
    "        english_ratio >= self.config.english_word_threshold or\n",
    "        vi_word_found or en_word_found or\n",
    "        len(text.split()) >= 2\n",
    "    )\n",
    "\n",
    "def _is_spam_content(self, text: str) -> bool:\n",
    "    \"\"\"Basic spam detection - can be overridden in subclasses\"\"\"\n",
    "    spam_score = 0\n",
    "    \n",
    "    # Promotional URLs\n",
    "    if re.search(r'http[s]?://(?:.*)(discount|sale|promo|buy|shop|deal)', text, re.IGNORECASE):\n",
    "        spam_score += 2\n",
    "    \n",
    "    # Promotional language\n",
    "    promo_patterns = [\n",
    "        r'\\b(click here|buy now|limited time|special offer|act now)\\b',\n",
    "        r'\\b(free shipping|50% off|discount code|coupon)\\b'\n",
    "    ]\n",
    "    for pattern in promo_patterns:\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            spam_score += 1\n",
    "    \n",
    "    # Excessive caps (more than 70% of text)\n",
    "    caps_ratio = len(re.findall(r'[A-Z]', text)) / len(text) if text else 0\n",
    "    if caps_ratio > 0.7 and len(text) > 10:\n",
    "        spam_score += 1\n",
    "    \n",
    "    # Very excessive punctuation (5+ consecutive)\n",
    "    if re.search(r'[!]{5,}', text):\n",
    "        spam_score += 1\n",
    "    \n",
    "    return spam_score >= 2\n",
    "\n",
    "# Add quality filtering methods to the AgentReadyPreprocessor class\n",
    "AgentReadyPreprocessor.quality_filter = quality_filter\n",
    "AgentReadyPreprocessor._has_excessive_repetition = _has_excessive_repetition\n",
    "AgentReadyPreprocessor._is_relevant_language = _is_relevant_language\n",
    "AgentReadyPreprocessor._is_spam_content = _is_spam_content\n",
    "\n",
    "print(\"Quality filtering methods added to AgentReadyPreprocessor\")\n",
    "print(\"Features: Length check, repetition detection, language relevance, spam detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bac1f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplication and privacy methods added to AgentReadyPreprocessor\n",
      "Features: Multi-level deduplication, n-gram similarity, PII redaction\n"
     ]
    }
   ],
   "source": [
    "def deduplicate(self, texts: List[str]) -> List[int]:\n",
    "    \"\"\"\n",
    "    ðŸ”„ Multi-level deduplication (LLM best practice)\n",
    "    \n",
    "    Implements three levels of deduplication:\n",
    "    1. Exact duplicate removal (hash-based)\n",
    "    2. Near-duplicate detection (n-gram similarity)\n",
    "    3. Cross-dataset deduplication\n",
    "    \n",
    "    Returns indices of unique texts to keep.\n",
    "    \"\"\"\n",
    "    unique_indices = []\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        # Exact duplicate check using hash\n",
    "        text_hash = hashlib.md5(text.encode()).hexdigest()\n",
    "        if text_hash in self.seen_hashes:\n",
    "            continue\n",
    "        \n",
    "        # Near-duplicate check using n-gram similarity\n",
    "        if self._is_near_duplicate(text):\n",
    "            continue\n",
    "        \n",
    "        # Add to unique set\n",
    "        self.seen_hashes.add(text_hash)\n",
    "        self._add_to_ngram_cache(text)\n",
    "        unique_indices.append(i)\n",
    "    \n",
    "    return unique_indices\n",
    "\n",
    "def _is_near_duplicate(self, text: str) -> bool:\n",
    "    \"\"\"Check for near-duplicates using n-gram similarity\"\"\"\n",
    "    text_ngrams = self._get_ngrams(text.lower(), self.config.ngram_size)\n",
    "    \n",
    "    for existing_ngrams in self.seen_ngrams.values():\n",
    "        similarity = self._calculate_ngram_similarity(text_ngrams, existing_ngrams)\n",
    "        if similarity >= self.config.similarity_threshold:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _get_ngrams(self, text: str, n: int) -> Set[str]:\n",
    "    \"\"\"Generate n-grams from text for similarity calculation\"\"\"\n",
    "    words = text.split()\n",
    "    if len(words) < n:\n",
    "        return set()\n",
    "    \n",
    "    ngrams = set()\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngram = ' '.join(words[i:i+n])\n",
    "        ngrams.add(ngram)\n",
    "    return ngrams\n",
    "\n",
    "def _calculate_ngram_similarity(self, ngrams1: Set[str], ngrams2: Set[str]) -> float:\n",
    "    \"\"\"Calculate Jaccard similarity between n-gram sets\"\"\"\n",
    "    if not ngrams1 or not ngrams2:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = len(ngrams1.intersection(ngrams2))\n",
    "    union = len(ngrams1.union(ngrams2))\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def _add_to_ngram_cache(self, text: str):\n",
    "    \"\"\"Add text n-grams to cache for duplicate detection\"\"\"\n",
    "    ngrams = self._get_ngrams(text.lower(), self.config.ngram_size)\n",
    "    self.seen_ngrams[text] = ngrams\n",
    "\n",
    "def redact_pii(self, text: str) -> str:\n",
    "    \"\"\"\n",
    "    Privacy redaction for production compliance (LLM best practice)\n",
    "    \n",
    "    Removes personally identifiable information:\n",
    "    - Email addresses\n",
    "    - Phone numbers (Vietnamese and international)\n",
    "    - URLs and web addresses\n",
    "    - Physical addresses\n",
    "    \"\"\"\n",
    "    if not self.config.enable_pii_redaction:\n",
    "        return text\n",
    "        \n",
    "    redacted_text = text\n",
    "        \n",
    "    # Apply each PII pattern\n",
    "    for pii_type, pattern in self.pii_patterns.items():\n",
    "        replacement = f\"[{pii_type.upper()}_REDACTED]\"\n",
    "        redacted_text = pattern.sub(replacement, redacted_text)\n",
    "        \n",
    "    return redacted_text\n",
    "\n",
    "# Add these methods to the AgentReadyPreprocessor class\n",
    "AgentReadyPreprocessor.deduplicate = deduplicate\n",
    "AgentReadyPreprocessor._is_near_duplicate = _is_near_duplicate\n",
    "AgentReadyPreprocessor._get_ngrams = _get_ngrams\n",
    "AgentReadyPreprocessor._calculate_ngram_similarity = _calculate_ngram_similarity\n",
    "AgentReadyPreprocessor._add_to_ngram_cache = _add_to_ngram_cache\n",
    "AgentReadyPreprocessor.redact_pii = redact_pii\n",
    "\n",
    "print(\"Deduplication and privacy methods added to AgentReadyPreprocessor\")\n",
    "print(\"Features: Multi-level deduplication, n-gram similarity, PII redaction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "130d5c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Text normalization and language processing methods added\n",
      " Normalization: Unicode NFKC, whitespace, character standardization\n",
      " Language detection: Vietnamese/English with confidence scores\n",
      " Abbreviation expansion: 150+ Vietnamese and English abbreviations\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(self, text: str) -> str:\n",
    "    \"\"\"\n",
    "    Text normalization following LLM preprocessing best practices normalization following LLM preprocessing\n",
    "\n",
    "    Implements Unicode NFKC normalization and consistent text formatting:\n",
    "    - Unicode character standardization\n",
    "    - Whitespace normalization\n",
    "    - Punctuation standardization\n",
    "    - Character encoding consistency\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Unicode normalization (NFKC - Normalization Form Canonical Composition)\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    \n",
    "    # Whitespace normalization\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Multiple spaces to single space\n",
    "    text = re.sub(r'\\n+', ' ', text)  # Multiple newlines to single space\n",
    "    text = re.sub(r'\\t+', ' ', text)  # Tabs to single space\n",
    "    \n",
    "    # Character normalization for consistent LLM input\n",
    "    # Standardize quotes\n",
    "    text = re.sub(r'[\"\"â€ž\"â€š'']', '\"', text)\n",
    "    text = re.sub(r'[''`Â´]', \"'\", text)\n",
    "    \n",
    "    # Standardize dashes\n",
    "    text = re.sub(r'[â€“â€”âˆ’]', '-', text)\n",
    "    \n",
    "    # Standardize ellipsis\n",
    "    text = re.sub(r'\\.{3,}', '...', text)\n",
    "    \n",
    "    # Remove excessive punctuation (keep emotional context)\n",
    "    text = re.sub(r'([!?]){3,}', r'\\1\\1', text)  # Limit to 2 repeated punctuation marks\n",
    "    \n",
    "    # Trim whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def detect_language(self, text: str) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    ðŸŒ Robust language detection with confidence scoring for Vietnamese and English\n",
    "    \n",
    "    Uses multiple detection strategies:\n",
    "    - Character-based detection for Vietnamese diacritics\n",
    "    - Word-based detection for common Vietnamese/English words\n",
    "    - Statistical analysis for confidence scoring\n",
    "    \"\"\"\n",
    "    if not text.strip():\n",
    "        return \"unknown\", 0.0\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Vietnamese character detection\n",
    "    vietnamese_chars = re.findall(r'[Ã Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]', text_lower)\n",
    "    vietnamese_ratio = len(vietnamese_chars) / len(text) if text else 0\n",
    "    \n",
    "    # Vietnamese word detection\n",
    "    vietnamese_words = ['vÃ ', 'cá»§a', 'cÃ³', 'nÃ y', 'cho', 'vá»›i', 'tá»«', 'Ä‘Æ°á»£c', 'má»™t', 'mÃ ', 'Ä‘á»ƒ', 'nhÆ°', 'tÃ´i', 'báº¡n', 'nÃ³', 'khÃ´ng', 'ráº¥t', 'sáº£n pháº©m']\n",
    "    vietnamese_word_count = sum(1 for word in vietnamese_words if word in text_lower)\n",
    "    \n",
    "    # English character/word detection\n",
    "    english_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "    english_ratio = len(english_words) / len(text.split()) if text.split() else 0\n",
    "    \n",
    "    # Common English words\n",
    "    common_english = ['the', 'and', 'of', 'to', 'a', 'in', 'for', 'is', 'on', 'that', 'by', 'this', 'with', 'i', 'you', 'it']\n",
    "    english_word_count = sum(1 for word in common_english if word in text_lower)\n",
    "    \n",
    "    # Decision logic with confidence scoring\n",
    "    if vietnamese_ratio > 0.05 or vietnamese_word_count > 0:\n",
    "        confidence = min(0.9, 0.6 + vietnamese_ratio * 2 + vietnamese_word_count * 0.1)\n",
    "        return \"vi\", confidence\n",
    "    elif english_ratio > 0.7 or english_word_count > 2:\n",
    "        confidence = min(0.9, 0.6 + english_ratio * 0.3 + english_word_count * 0.05)\n",
    "        return \"en\", confidence\n",
    "    elif english_ratio > 0.3:\n",
    "        return \"en\", 0.6\n",
    "    else:\n",
    "        return \"mixed\", 0.5\n",
    "\n",
    "def expand_abbreviations(self, text: str, language: str) -> str:\n",
    "    \"\"\"\n",
    "    ðŸ”¤ Context-aware abbreviation expansion for Vietnamese and English\n",
    "    \n",
    "    Expands 150+ abbreviations to provide clearer context for LLM sentiment analysis.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    expanded_words = []\n",
    "    \n",
    "    # Choose appropriate abbreviation dictionary\n",
    "    if language == \"vi\":\n",
    "        abbrev_dict = {**self.vietnamese_abbrev, **self.english_abbrev}\n",
    "    else:\n",
    "        abbrev_dict = self.english_abbrev\n",
    "    \n",
    "    for word in words:\n",
    "        # Clean word for matching (preserve punctuation context)\n",
    "        clean_word = re.sub(r'[^\\w]', '', word.lower())\n",
    "        \n",
    "        if clean_word in abbrev_dict:\n",
    "            # Preserve original capitalization pattern\n",
    "            expansion = abbrev_dict[clean_word]\n",
    "            if word[0].isupper():\n",
    "                expansion = expansion.capitalize()\n",
    "            expanded_words.append(expansion)\n",
    "        else:\n",
    "            expanded_words.append(word)\n",
    "    \n",
    "    return ' '.join(expanded_words)\n",
    "\n",
    "# Add methods to the class\n",
    "AgentReadyPreprocessor.normalize_text = normalize_text\n",
    "AgentReadyPreprocessor.detect_language = detect_language\n",
    "AgentReadyPreprocessor.expand_abbreviations = expand_abbreviations\n",
    "\n",
    "print(\" Text normalization and language processing methods added\")\n",
    "print(\" Normalization: Unicode NFKC, whitespace, character standardization\")\n",
    "print(\" Language detection: Vietnamese/English with confidence scores\")\n",
    "print(\" Abbreviation expansion: 150+ Vietnamese and English abbreviations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3eb5d210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Core processing pipeline and data loading methods added\n",
      " Pipeline: Quality filter â†’ Privacy redaction â†’ Normalization â†’ Language detection â†’ Abbreviation expansion\n",
      " Data loading: Supports Tiki and YouTube JSON formats\n"
     ]
    }
   ],
   "source": [
    "def process_single_item(self, text: str, metadata: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    ðŸ”„ Process a single text item through the complete LLM preprocessing pipeline\n",
    "    \n",
    "    Pipeline stages:\n",
    "    1. Quality filtering â†’ 2. Privacy redaction â†’ 3. Text normalization \n",
    "    â†’ 4. Language detection â†’ 5. Abbreviation expansion â†’ 6. Metadata cleaning\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Quality filtering\n",
    "        quality_passed, quality_reason = self.quality_filter(text, metadata)\n",
    "        if not quality_passed:\n",
    "            self.stats['filtered_quality'] += 1\n",
    "            return None\n",
    "        \n",
    "        # Step 2: Privacy redaction\n",
    "        redacted_text = self.redact_pii(text)\n",
    "        if redacted_text != text:\n",
    "            self.stats['filtered_privacy'] += 1\n",
    "        \n",
    "        # Step 3: Text normalization\n",
    "        normalized_text = self.normalize_text(redacted_text)\n",
    "        \n",
    "        # Step 4: Language detection\n",
    "        language, confidence = self.detect_language(normalized_text)\n",
    "        \n",
    "        # Step 5: Abbreviation expansion\n",
    "        expanded_text = self.expand_abbreviations(normalized_text, language)\n",
    "        \n",
    "        # Step 6: Clean metadata\n",
    "        cleaned_metadata = self._clean_metadata(metadata)\n",
    "        \n",
    "        # Update statistics\n",
    "        self.stats['language_distribution'][language] = self.stats['language_distribution'].get(language, 0) + 1\n",
    "        self.stats['processed_successfully'] += 1\n",
    "        \n",
    "        # Create agent-ready item\n",
    "        processed_item = {\n",
    "            \"id\": self.stats['processed_successfully'],\n",
    "            \"original_text\": text,\n",
    "            \"cleaned_text\": expanded_text,\n",
    "            \"language\": language,\n",
    "            \"language_confidence\": confidence,\n",
    "            \"processing_steps\": {\n",
    "                \"quality_filter\": \"passed\",\n",
    "                \"privacy_redaction\": \"applied\" if redacted_text != text else \"not_needed\",\n",
    "                \"text_normalization\": \"applied\",\n",
    "                \"language_detection\": f\"{language} ({confidence:.2f})\",\n",
    "                \"abbreviation_expansion\": \"applied\"\n",
    "            },\n",
    "            \"metadata\": cleaned_metadata\n",
    "        }\n",
    "        \n",
    "        return processed_item\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing item: {str(e)}\")\n",
    "        self.stats['processing_errors'] += 1\n",
    "        return None\n",
    "    \n",
    "def _clean_metadata(self, metadata: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Clean and standardize metadata for agent consumption\"\"\"\n",
    "    cleaned = {}\n",
    "    \n",
    "    for key, value in metadata.items():\n",
    "        # Handle NaN and missing values\n",
    "        if pd.isna(value) or value is None or value == 'null':\n",
    "            if 'id' in key.lower():\n",
    "                cleaned[key] = None\n",
    "            elif any(keyword in key.lower() for keyword in ['title', 'content', 'name']):\n",
    "                cleaned[key] = \"\"\n",
    "            elif any(keyword in key.lower() for keyword in ['count', 'rating', 'score']):\n",
    "                cleaned[key] = 0\n",
    "            else:\n",
    "                cleaned[key] = None\n",
    "        else:\n",
    "            # Clean string values\n",
    "            if isinstance(value, str):\n",
    "                cleaned[key] = value.strip()\n",
    "            else:\n",
    "                cleaned[key] = value\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def load_data(self, file_paths: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    ðŸ“‚ Load data from multiple JSON files with robust error handling\n",
    "    \n",
    "    Handles different data structures:\n",
    "    - Tiki format: {\"product_name\": [reviews]}\n",
    "    - YouTube format: [{\"video_title\": \"\", \"content\": \"\"}]\n",
    "    \"\"\"\n",
    "    all_items = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            logger.info(f\"Loading data from: {file_path}\")\n",
    "            \n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Determine source type\n",
    "            source = 'tiki' if 'tiki' in file_path.lower() else 'youtube'\n",
    "            \n",
    "            # Process different data structures\n",
    "            if isinstance(data, dict):\n",
    "                # Tiki format: {\"product_name\": [reviews]}\n",
    "                for product_name, reviews in data.items():\n",
    "                    if isinstance(reviews, list):\n",
    "                        for review in reviews:\n",
    "                            if isinstance(review, dict) and review.get('content'):\n",
    "                                review['source'] = source\n",
    "                                review['product_name'] = product_name\n",
    "                                all_items.append(review)\n",
    "            \n",
    "            elif isinstance(data, list):\n",
    "                # YouTube format: [{\"video_title\": \"\", \"content\": \"\"}]\n",
    "                for item in data:\n",
    "                    if isinstance(item, dict) and item.get('content'):\n",
    "                        item['source'] = source\n",
    "                        all_items.append(item)\n",
    "            \n",
    "            # Update source statistics\n",
    "            source_count = len([item for item in all_items if item.get('source') == source])\n",
    "            self.stats['source_distribution'][source] = source_count\n",
    "            \n",
    "            logger.info(f\"Loaded {source_count} items from {source}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    self.stats['total_loaded'] = len(all_items)\n",
    "    logger.info(f\"ðŸ“Š Total items loaded: {len(all_items)}\")\n",
    "    \n",
    "    return all_items\n",
    "\n",
    "# Add methods to the class\n",
    "AgentReadyPreprocessor.process_single_item = process_single_item\n",
    "AgentReadyPreprocessor._clean_metadata = _clean_metadata\n",
    "AgentReadyPreprocessor.load_data = load_data\n",
    "\n",
    "print(\" Core processing pipeline and data loading methods added\")\n",
    "print(\" Pipeline: Quality filter â†’ Privacy redaction â†’ Normalization â†’ Language detection â†’ Abbreviation expansion\")\n",
    "print(\" Data loading: Supports Tiki and YouTube JSON formats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d779a7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete LLM preprocessing pipeline ready!\n",
      "Pipeline stages: Load â†’ Deduplicate â†’ Process â†’ Save â†’ Report\n",
      "Output: Agent-ready JSON for LLM sentiment analysis\n",
      "Features: Quality metrics, processing statistics, error handling\n"
     ]
    }
   ],
   "source": [
    "def process_dataset(self, file_paths: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Execute the complete LLM preprocessing pipeline\n",
    "\n",
    "    Full pipeline execution:\n",
    "    1. Load data from multiple sources\n",
    "    2. Multi-level deduplication\n",
    "    3. Individual item processing\n",
    "    4. Agent-ready output generation\n",
    "    5. Statistics and quality reporting\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting advanced LLM preprocessing pipeline\")\n",
    "    \n",
    "    # Step 1: Load data\n",
    "    all_items = self.load_data(file_paths)\n",
    "    \n",
    "    if not all_items:\n",
    "        logger.error(\"No data loaded. Exiting.\")\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Extract texts for deduplication\n",
    "    texts = [item.get('content', '') for item in all_items]\n",
    "    \n",
    "    # Step 3: Multi-level deduplication\n",
    "    logger.info(\"Performing multi-level deduplication...\")\n",
    "    unique_indices = self.deduplicate(texts)\n",
    "    self.stats['filtered_duplicates'] = len(all_items) - len(unique_indices)\n",
    "    \n",
    "    # Step 4: Process remaining items\n",
    "    logger.info(\"Processing individual items through pipeline...\")\n",
    "    processed_items = []\n",
    "    \n",
    "    for idx in unique_indices:\n",
    "        item = all_items[idx]\n",
    "        text = item.get('content', '')\n",
    "        \n",
    "        processed_item = self.process_single_item(text, item)\n",
    "        if processed_item:\n",
    "            processed_items.append(processed_item)\n",
    "    \n",
    "    # Step 5: Save results\n",
    "    logger.info(\"Saving agent-ready data...\")\n",
    "    output_path = self._save_agent_ready_data(processed_items)\n",
    "    \n",
    "    # Step 6: Generate comprehensive report\n",
    "    self._generate_comprehensive_report()\n",
    "    \n",
    "    logger.info(f\"Preprocessing complete. Output saved to: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "def _save_agent_ready_data(self, processed_items: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Save data in agent-ready format for LLM sentiment analysis\n",
    "\n",
    "    Creates structured JSON with:\n",
    "    - Preprocessing metadata\n",
    "    - Quality metrics\n",
    "    - Processing statistics\n",
    "    - Clean, normalized data\n",
    "    \"\"\"\n",
    "    os.makedirs(self.config.output_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_path = os.path.join(self.config.output_dir, f\"agent_ready_sentiment_data_{timestamp}.json\")\n",
    "    \n",
    "    # Create comprehensive agent-ready structure\n",
    "    agent_data = {\n",
    "        \"preprocessing_metadata\": {\n",
    "            \"version\": \"2.0_LLM_optimized\",\n",
    "            \"processing_timestamp\": datetime.now().isoformat(),\n",
    "            \"total_items\": len(processed_items),\n",
    "            \"preprocessing_pipeline\": [\n",
    "                \"quality_filtering\",\n",
    "                \"multi_level_deduplication\", \n",
    "                \"privacy_redaction\",\n",
    "                \"text_normalization\",\n",
    "                \"language_detection\",\n",
    "                \"abbreviation_expansion\"\n",
    "            ],\n",
    "            \"config\": {\n",
    "                \"min_text_length\": self.config.min_text_length,\n",
    "                \"max_text_length\": self.config.max_text_length,\n",
    "                \"similarity_threshold\": self.config.similarity_threshold,\n",
    "                \"privacy_redaction_enabled\": self.config.enable_pii_redaction\n",
    "            }\n",
    "        },\n",
    "        \"quality_metrics\": {\n",
    "            \"total_loaded\": self.stats['total_loaded'],\n",
    "            \"successfully_processed\": self.stats['processed_successfully'],\n",
    "            \"quality_filtered\": self.stats['filtered_quality'],\n",
    "            \"duplicates_removed\": self.stats['filtered_duplicates'],\n",
    "            \"privacy_redactions\": self.stats['filtered_privacy'],\n",
    "            \"processing_errors\": self.stats['processing_errors'],\n",
    "            \"success_rate\": (self.stats['processed_successfully'] / self.stats['total_loaded']) * 100 if self.stats['total_loaded'] > 0 else 0\n",
    "        },\n",
    "        \"data_distribution\": {\n",
    "            \"source_distribution\": self.stats['source_distribution'],\n",
    "            \"language_distribution\": self.stats['language_distribution']\n",
    "        },\n",
    "        \"sentiment_analysis_ready_data\": processed_items\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(agent_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def _generate_comprehensive_report(self):\n",
    "    \"\"\"Generate detailed preprocessing report for analysis\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    report_path = os.path.join(self.config.output_dir, f\"preprocessing_report_{timestamp}.txt\")\n",
    "    \n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"LLM-OPTIMIZED PREPROCESSING REPORT\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Processing Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Purpose: Agent-ready data for LLM sentiment analysis\\n\\n\")\n",
    "        \n",
    "        f.write(\"INPUT STATISTICS:\\n\")\n",
    "        f.write(f\"   â€¢ Total items loaded: {self.stats['total_loaded']}\\n\")\n",
    "        f.write(f\"   â€¢ Source distribution: {self.stats['source_distribution']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"FILTERING STATISTICS:\\n\")\n",
    "        f.write(f\"   â€¢ Quality filter removed: {self.stats['filtered_quality']} items\\n\")\n",
    "        f.write(f\"   â€¢ Deduplication removed: {self.stats['filtered_duplicates']} items\\n\")\n",
    "        f.write(f\"   â€¢ Privacy redaction applied: {self.stats['filtered_privacy']} items\\n\")\n",
    "        f.write(f\"   â€¢ Processing errors: {self.stats['processing_errors']} items\\n\\n\")\n",
    "        \n",
    "        f.write(\"OUTPUT STATISTICS:\\n\")\n",
    "        f.write(f\"   â€¢ Successfully processed: {self.stats['processed_successfully']} items\\n\")\n",
    "        f.write(f\"   â€¢ Language distribution: {self.stats['language_distribution']}\\n\")\n",
    "        \n",
    "        success_rate = (self.stats['processed_successfully'] / self.stats['total_loaded']) * 100 if self.stats['total_loaded'] > 0 else 0\n",
    "        f.write(f\"   â€¢ Overall success rate: {success_rate:.2f}%\\n\\n\")\n",
    "        \n",
    "        f.write(\"LLM AGENT READINESS:\\n\")\n",
    "        f.write(\"   Quality filtering applied (removes noise)\\n\")\n",
    "        f.write(\"   Deduplication completed (prevents bias)\\n\")\n",
    "        f.write(\"   Privacy redaction enabled (production safe)\\n\")\n",
    "        f.write(\"   Text normalization applied (consistent input)\\n\")\n",
    "        f.write(\"   Language detection completed (context aware)\\n\")\n",
    "        f.write(\"   Abbreviation expansion applied (clear meaning)\\n\")\n",
    "    \n",
    "    logger.info(f\"Comprehensive report saved to: {report_path}\")\n",
    "\n",
    "# Add final methods to the class\n",
    "AgentReadyPreprocessor.process_dataset = process_dataset\n",
    "AgentReadyPreprocessor._save_agent_ready_data = _save_agent_ready_data\n",
    "AgentReadyPreprocessor._generate_comprehensive_report = _generate_comprehensive_report\n",
    "\n",
    "print(\"Complete LLM preprocessing pipeline ready!\")\n",
    "print(\"Pipeline stages: Load â†’ Deduplicate â†’ Process â†’ Save â†’ Report\")\n",
    "print(\"Output: Agent-ready JSON for LLM sentiment analysis\")\n",
    "print(\"Features: Quality metrics, processing statistics, error handling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c96c47b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded. Ready to process data.\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”§ CONFIGURATION AND EXECUTION\n",
    "\n",
    "# Configuration for preprocessing\n",
    "config = PreprocessingConfig()\n",
    "\n",
    "# Input files (your data)\n",
    "input_files = [\n",
    "    \"tiki_airpod_reviews.json\",\n",
    "    \"youtube_airpod_20250629_032359.json\",\n",
    "    \"youtube_airpod_english_review_20250629_032835.json\"\n",
    "]\n",
    "\n",
    "print(\"Configuration loaded. Ready to process data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20e329c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 04:55:52,941 - INFO - ðŸ¤– Agent-Ready Preprocessor initialized with LLM best practices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 04:55:52,951 - INFO - Starting advanced LLM preprocessing pipeline\n",
      "2025-07-01 04:55:52,955 - INFO - Loading data from: tiki_airpod_reviews.json\n",
      "2025-07-01 04:55:52,955 - INFO - Loading data from: tiki_airpod_reviews.json\n",
      "2025-07-01 04:55:52,959 - INFO - Loaded 69 items from tiki\n",
      "2025-07-01 04:55:52,967 - INFO - Loading data from: youtube_airpod_20250629_032359.json\n",
      "2025-07-01 04:55:52,977 - INFO - Loaded 100 items from youtube\n",
      "2025-07-01 04:55:52,959 - INFO - Loaded 69 items from tiki\n",
      "2025-07-01 04:55:52,967 - INFO - Loading data from: youtube_airpod_20250629_032359.json\n",
      "2025-07-01 04:55:52,977 - INFO - Loaded 100 items from youtube\n",
      "2025-07-01 04:55:52,988 - INFO - Loading data from: youtube_airpod_english_review_20250629_032835.json\n",
      "2025-07-01 04:55:52,995 - INFO - Loaded 150 items from youtube\n",
      "2025-07-01 04:55:52,988 - INFO - Loading data from: youtube_airpod_english_review_20250629_032835.json\n",
      "2025-07-01 04:55:52,995 - INFO - Loaded 150 items from youtube\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Starting LLM-optimized preprocessing pipeline...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 04:55:53,001 - INFO - ðŸ“Š Total items loaded: 219\n",
      "2025-07-01 04:55:53,007 - INFO - Performing multi-level deduplication...\n",
      "2025-07-01 04:55:53,007 - INFO - Performing multi-level deduplication...\n",
      "2025-07-01 04:55:53,603 - INFO - Processing individual items through pipeline...\n",
      "2025-07-01 04:55:53,603 - INFO - Processing individual items through pipeline...\n",
      "2025-07-01 04:55:54,195 - INFO - Saving agent-ready data...\n",
      "2025-07-01 04:55:54,195 - INFO - Saving agent-ready data...\n",
      "2025-07-01 04:55:54,440 - INFO - Comprehensive report saved to: agent_ready_data\\preprocessing_report_20250701_045554.txt\n",
      "2025-07-01 04:55:54,450 - INFO - Preprocessing complete. Output saved to: agent_ready_data\\agent_ready_sentiment_data_20250701_045554.json\n",
      "2025-07-01 04:55:54,440 - INFO - Comprehensive report saved to: agent_ready_data\\preprocessing_report_20250701_045554.txt\n",
      "2025-07-01 04:55:54,450 - INFO - Preprocessing complete. Output saved to: agent_ready_data\\agent_ready_sentiment_data_20250701_045554.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Preprocessing complete. Output saved to: agent_ready_data\\agent_ready_sentiment_data_20250701_045554.json\n"
     ]
    }
   ],
   "source": [
    "# ðŸš€ EXECUTE LLM PREPROCESSING PIPELINE\n",
    "\n",
    "# Enhanced preprocessor with balanced spam detection\n",
    "class BalancedPreprocessor(AgentReadyPreprocessor):\n",
    "    \"\"\"Improved preprocessor with balanced filtering for better success rates\"\"\"\n",
    "    \n",
    "    def _is_spam_content(self, text: str) -> bool:\n",
    "        \"\"\"Balanced spam detection - allows emotional expressions\"\"\"\n",
    "        spam_score = 0\n",
    "        \n",
    "        # Promotional URLs\n",
    "        if re.search(r'http[s]?://(?:.*)(discount|sale|promo|buy|shop|deal)', text, re.IGNORECASE):\n",
    "            spam_score += 2\n",
    "        \n",
    "        # Promotional language\n",
    "        promo_patterns = [\n",
    "            r'\\b(click here|buy now|limited time|special offer|act now)\\b',\n",
    "            r'\\b(free shipping|50% off|discount code|coupon)\\b'\n",
    "        ]\n",
    "        for pattern in promo_patterns:\n",
    "            if re.search(pattern, text, re.IGNORECASE):\n",
    "                spam_score += 1\n",
    "        \n",
    "        # Excessive caps (more than 50% of text)\n",
    "        caps_ratio = len(re.findall(r'[A-Z]', text)) / len(text) if text else 0\n",
    "        if caps_ratio > 0.5 and len(text) > 20:\n",
    "            spam_score += 1\n",
    "        \n",
    "        # Only flag 5+ consecutive exclamation marks (allow emotion)\n",
    "        if re.search(r'[!]{5,}', text):\n",
    "            spam_score += 1\n",
    "        \n",
    "        return spam_score >= 2\n",
    "    \n",
    "    def _is_relevant_language(self, text: str) -> bool:\n",
    "        \"\"\"More inclusive language detection\"\"\"\n",
    "        # Vietnamese detection\n",
    "        vietnamese_chars = re.findall(r'[Ã Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]', text.lower())\n",
    "        vietnamese_ratio = len(vietnamese_chars) / len(text) if text else 0\n",
    "        \n",
    "        # English detection\n",
    "        english_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "        english_ratio = len(english_words) / len(text.split()) if text.split() else 0\n",
    "        \n",
    "        # Common words check\n",
    "        vi_words = ['vÃ ', 'cá»§a', 'cÃ³', 'nÃ y', 'cho', 'vá»›i', 'tá»«', 'Ä‘Æ°á»£c', 'má»™t', 'khÃ´ng', 'ráº¥t', 'sáº£n pháº©m', 'tá»‘t', 'áº¡']\n",
    "        en_words = ['the', 'and', 'good', 'bad', 'great', 'nice', 'quality', 'product', 'sound', 'battery']\n",
    "        \n",
    "        vi_word_found = any(word in text.lower() for word in vi_words)\n",
    "        en_word_found = any(word in text.lower() for word in en_words)\n",
    "        \n",
    "        return (\n",
    "            vietnamese_ratio >= self.config.vietnamese_char_threshold or \n",
    "            english_ratio >= self.config.english_word_threshold or\n",
    "            vi_word_found or en_word_found or\n",
    "            len(text.split()) >= 2\n",
    "        )\n",
    "\n",
    "# Initialize balanced preprocessor\n",
    "preprocessor = BalancedPreprocessor(config)\n",
    "\n",
    "print(\"ðŸ¤– Starting LLM-optimized preprocessing pipeline...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Execute pipeline\n",
    "output_path = preprocessor.process_dataset(input_files)\n",
    "\n",
    "# Output results\n",
    "print(f\"âœ… Preprocessing complete. Output saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73af380",
   "metadata": {},
   "source": [
    "# Usage Guide\n",
    "\n",
    "## Loading Preprocessed Data\n",
    "```python\n",
    "import json\n",
    "\n",
    "# Load agent-ready data\n",
    "with open('agent_ready_data/agent_ready_sentiment_data_YYYYMMDD_HHMMSS.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Access clean data for sentiment analysis\n",
    "for item in data['sentiment_analysis_ready_data']:\n",
    "    text = item['cleaned_text']           # Clean, normalized text\n",
    "    language = item['language']           # 'vi' or 'en' \n",
    "    confidence = item['language_confidence']  # Detection confidence\n",
    "    source = item['metadata']['source']   # 'tiki' or 'youtube'\n",
    "    \n",
    "    # Run your sentiment analysis here\n",
    "    sentiment = your_sentiment_model(text, language=language)\n",
    "```\n",
    "\n",
    "## Redaction Tag Meanings\n",
    "- `[EMAIL_REDACTED]` - Email addresses removed for privacy\n",
    "- `[PHONE_VN_REDACTED]` - Vietnamese phone numbers removed  \n",
    "- `[PHONE_INTL_REDACTED]` - International phone numbers removed\n",
    "- `[URL_REDACTED]` - Web URLs removed for security\n",
    "- `[ADDRESS_REDACTED]` - Physical addresses removed\n",
    "\n",
    "**Your data is now ready for accurate LLM-based sentiment analysis! ðŸ¤–**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d549c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 04:55:54,590 - INFO - ðŸ¤– Agent-Ready Preprocessor initialized with LLM best practices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing improved PII redaction patterns:\n",
      "==================================================\n",
      "1. --PRESERVED\n",
      "   Original: Sáº£n pháº©m tá»‘t, Ä‘Ã¡nh giÃ¡ 8.5/10 Ä‘iá»ƒm\n",
      "\n",
      "2. --PRESERVED\n",
      "   Original: Äiá»ƒm sá»‘: 9/10, ráº¥t hÃ i lÃ²ng\n",
      "\n",
      "3. --REDACTED\n",
      "   Original: LiÃªn há»‡ tÃ´i qua sá»‘ 0123456789\n",
      "   Redacted: LiÃªn há»‡ tÃ´i qua sá»‘ [PHONE_VN_REDACTED]\n",
      "\n",
      "4. --REDACTED\n",
      "   Original: Email me at test@example.com\n",
      "   Redacted: Email me at [EMAIL_REDACTED]\n",
      "\n",
      "5. --PRESERVED\n",
      "   Original: Rating: 4.5/5 stars, good quality\n",
      "\n",
      "6. --REDACTED\n",
      "   Original: Call +84987654321 for support\n",
      "   Redacted: Call [PHONE_VN_REDACTED] for support\n",
      "\n",
      "PII redaction test complete!\n",
      "Ratings like '8.5/10' should be PRESERVED\n",
      "Phone numbers and emails should be REDACTED\n"
     ]
    }
   ],
   "source": [
    "# ðŸ§ª TEST: Verify improved PII redaction\n",
    "test_preprocessor = AgentReadyPreprocessor(config)\n",
    "\n",
    "test_texts = [\n",
    "    \"Sáº£n pháº©m tá»‘t, Ä‘Ã¡nh giÃ¡ 8.5/10 Ä‘iá»ƒm\",  # Should NOT be redacted\n",
    "    \"Äiá»ƒm sá»‘: 9/10, ráº¥t hÃ i lÃ²ng\",  # Should NOT be redacted  \n",
    "    \"LiÃªn há»‡ tÃ´i qua sá»‘ 0123456789\",  # SHOULD be redacted\n",
    "    \"Email me at test@example.com\",  # SHOULD be redacted\n",
    "    \"Rating: 4.5/5 stars, good quality\",  # Should NOT be redacted\n",
    "    \"Call +84987654321 for support\"  # SHOULD be redacted\n",
    "]\n",
    "\n",
    "print(\"Testing improved PII redaction patterns:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    redacted = test_preprocessor.redact_pii(text)\n",
    "    is_changed = redacted != text\n",
    "    status = \"--REDACTED\" if is_changed else \"--PRESERVED\"\n",
    "    \n",
    "    print(f\"{i}. {status}\")\n",
    "    print(f\"   Original: {text}\")\n",
    "    if is_changed:\n",
    "        print(f\"   Redacted: {redacted}\")\n",
    "    print()\n",
    "\n",
    "print(\"PII redaction test complete!\")\n",
    "print(\"Ratings like '8.5/10' should be PRESERVED\")\n",
    "print(\"Phone numbers and emails should be REDACTED\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
