#!/usr/bin/env python3
"""
Advanced preprocessor for scraped data with review filtering
"""

import re
import hashlib
import logging
from typing import List, Dict, Any, Set, Tuple
from .config import PreprocessingConfig

logger = logging.getLogger(__name__)

class AdvancedPreprocessor:
    """Advanced preprocessing pipeline for scraped data"""
    
    def __init__(self, config: PreprocessingConfig):
        self.config = config
        self.stats = {
            'total_loaded': 0,
            'filtered_quality': 0,
            'filtered_duplicates': 0,
            'filtered_non_review': 0,
            'filtered_spam': 0,
            'processed_successfully': 0,
            'language_distribution': {},
            'source_distribution': {}
        }
        
        # Cache for duplicate detection
        self.seen_hashes: Set[str] = set()
        
        # Initialize review filtering patterns
        self._init_review_filters()
        
    def _calculate_text_hash(self, text: str) -> str:
        """Calculate hash for duplicate detection"""
        # Normalize text for hashing
        normalized = re.sub(r'\s+', ' ', text.lower().strip())
        return hashlib.md5(normalized.encode()).hexdigest()
    
    def _is_quality_text(self, text: str) -> bool:
        """Check if text meets quality requirements"""
        if not text or not text.strip():
            return False
        
        text = text.strip()
        
        # Length checks
        if len(text) < self.config.min_text_length or len(text) > self.config.max_text_length:
            return False
        
        # Word count check
        words = text.split()
        if len(words) < self.config.min_word_count:
            return False
        
        # Repetition check
        if len(words) > 1:
            unique_words = set(words)
            repetition_ratio = 1 - (len(unique_words) / len(words))
            if repetition_ratio > self.config.max_repetition_ratio:
                return False
        
        return True
    
    def _detect_language(self, text: str) -> str:
        """Simple language detection for Vietnamese/English"""
        if not text:
            return 'unknown'
        
        # Vietnamese character patterns
        vietnamese_chars = r'[Ã Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘Ä]'
        vietnamese_count = len(re.findall(vietnamese_chars, text, re.IGNORECASE))
        vietnamese_ratio = vietnamese_count / len(text) if text else 0
        
        if vietnamese_ratio >= self.config.vietnamese_char_threshold:
            return 'vi'
        
        # English word detection
        english_words = re.findall(r'\b[a-zA-Z]+\b', text)
        english_ratio = len(english_words) / len(text.split()) if text.split() else 0
        
        if english_ratio >= self.config.english_word_threshold:
            return 'en'
        
        return 'mixed'
    
    def _normalize_text(self, text: str) -> str:
        """Normalize text content"""
        if not text:
            return ''
        
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text.strip())
        
        # Remove excessive punctuation
        text = re.sub(r'[!]{2,}', '!', text)
        text = re.sub(r'[?]{2,}', '?', text)
        text = re.sub(r'[.]{3,}', '...', text)
        
        # Basic text cleanup
        text = re.sub(r'https?://\S+', '[URL]', text)  # Replace URLs
        text = re.sub(r'\d{10,}', '[PHONE]', text)     # Replace phone numbers
        
        return text.strip()
    
    def _init_review_filters(self):
        """Initialize patterns for identifying non-review comments"""
        
        # Patterns for non-review content (Vietnamese and English)
        self.non_review_patterns = {
            'greeting_only': [
                r'^(hi|hello|hey|chÃ o|xin chÃ o|helo)\s*[!.]*\s*$',
                r'^(thanks?|thank you|cáº£m Æ¡n|cam on|tks|ty)\s*[!.]*\s*$',
                r'^(bye|goodbye|táº¡m biá»‡t|chÃ o táº¡m biá»‡t)\s*[!.]*\s*$'
            ],
            'short_reactions': [
                r'^(ok|okay|okie|oke|good|nice|tá»‘t|hay|Ä‘Æ°á»£c)\s*[!.]*\s*$',
                r'^(wow|amazing|tuyá»‡t|Ä‘á»‰nh|cool|ngon)\s*[!.]*\s*$',
                r'^(yes|no|cÃ³|khÃ´ng|á»«|uhm|hmm)\s*[!.]*\s*$'
            ],
            'spam_patterns': [
                r'(like vÃ  subscribe|like sub|Ä‘Äƒng kÃ½ kÃªnh)',
                r'(inbox|pm|liÃªn há»‡|contact|phone|zalo|facebook)',
                r'(bÃ¡n|mua|sell|buy|giÃ¡ ráº»|cheap|discount|khuyáº¿n mÃ£i)',
                r'(link|website|web|shop|store|cá»­a hÃ ng)',
                r'(\d{10,}|\d{3,4}[-.\s]\d{3,4}[-.\s]\d{3,4})',  # Phone numbers
                r'(facebook\.com|fb\.com|zalo|telegram|whatsapp)'
            ],
            'question_seeking_help': [
                r'^(help|giÃºp|há»i|ask|question|cÃ¢u há»i)',
                r'(how to|lÃ m sao|lÃ m tháº¿ nÃ o|hÆ°á»›ng dáº«n|guide)',
                r'(where|Ä‘Ã¢u|á»Ÿ Ä‘Ã¢u|tÃ¬m á»Ÿ Ä‘Ã¢u|mua á»Ÿ Ä‘Ã¢u)',
                r'(when|khi nÃ o|bao giá»|lÃºc nÃ o)'
            ],
            'off_topic': [
                r'(first|Ä‘áº§u tiÃªn|1st|Ä‘áº§u|ai xem Ä‘áº§u)',
                r'(early|sá»›m|view sá»›m|xem sá»›m)',
                r'(music|nháº¡c|bÃ i hÃ¡t|song|beat)',
                r'(movie|phim|film|video clip|mv)'
            ],
            'emotional_only': [
                r'^[ðŸ˜€-ðŸ™ðŸ¤”-ðŸ¤¯ðŸ˜­-ðŸ˜±ðŸ¥°-ðŸ¥µ]+\s*$',  # Only emojis
                r'^(haha|hihi|huhu|hoho)+\s*[!.]*\s*$',
                r'^(lol|lmao|omg|wtf|wth)\s*[!.]*\s*$'
            ]
        }
        
        # Patterns that indicate genuine reviews
        self.review_indicators = [
            r'(sá»­ dá»¥ng|dÃ¹ng|use|used|using)',
            r'(cháº¥t lÆ°á»£ng|quality|tá»‘t|xáº¥u|good|bad|nice)',
            r'(giÃ¡|price|cost|expensive|cheap|ráº»|Ä‘áº¯t)',
            r'(mua|buy|bought|purchase|order|Ä‘áº·t hÃ ng)',
            r'(giao hÃ ng|ship|delivery|nháº­n hÃ ng|packaging)',
            r'(recommend|Ä‘á» xuáº¥t|khuyÃªn|should|nÃªn)',
            r'(experience|tráº£i nghiá»‡m|cáº£m nháº­n|feel|cáº£m giÃ¡c)',
            r'(so sÃ¡nh|compare|comparison|khÃ¡c|different)',
            r'(Ä‘Ã¡nh giÃ¡|review|rate|rating|star|sao)',
            r'(Æ°u Ä‘iá»ƒm|nhÆ°á»£c Ä‘iá»ƒm|pros|cons|advantage|disadvantage)',
            r'(mÃ u sáº¯c|color|size|kÃ­ch thÆ°á»›c|design|thiáº¿t káº¿)',
            r'(pin|battery|charge|sáº¡c|Ä‘iá»‡n)',
            r'(camera|áº£nh|photo|picture|video|quay)',
            r'(Ã¢m thanh|sound|speaker|loa|music|nháº¡c)'
        ]
        
        # Enhanced patterns for strict product experience filtering
        self.product_experience_indicators = [
            r'(Ä‘Ã£ mua|Ä‘Ã£ dÃ¹ng|Ä‘Ã£ thá»­|bought|used|tried)',
            r'(lÃªn mÃ´i|apply|wear|khi dÃ¹ng|when using|after using)',
            r'(cháº¥t son|texture|Ä‘á»™ bá»n|lasting|smudge|fade)',
            r'(khÃ´ mÃ´i|dry lips|moisturizing|dÆ°á»¡ng áº©m|comfortable)',
            r'(pigment|mÃ u sáº¯c thá»±c táº¿|actual color|lÃªn mÃ u|color payoff)',
            r'(tráº£i nghiá»‡m thá»±c táº¿|actual experience|thá»±c sá»±|really|actually)',
            r'(sau khi|after|before|trÆ°á»›c khi|trong quÃ¡ trÃ¬nh|during)',
            r'(cáº£m giÃ¡c|feel|feeling|touch|cáº§m náº¯m|grip)',
            r'(hiá»‡u suáº¥t|performance|tá»‘c Ä‘á»™|speed|lag|smooth)',
            r'(pin|battery life|sáº¡c|charging|runtime|standby)'
        ]
        
        # Patterns for non-product content (videos, marketing, etc.)
        self.non_product_content_patterns = [
            r'(video|vid|swatch|clip|recording|filming|camera|youtube|channel)',
            r'(giveaway|ga|ctkm|khuyáº¿n mÃ£i|táº·ng|mua|giáº£m giÃ¡|promotion)',
            r'(thank|cáº£m Æ¡n|thanks|thÃ­ch video|love video|subscribe)',
            r'(Ã¡o|vÃ¡y|outfit|trang phá»¥c|clothes|fashion)',
            r'(info|infor|link|http|bit\.ly|shop|mua á»Ÿ Ä‘Ã¢u|where to buy)',
            r'(mÃ u nÃ o|which color|chá»n mÃ u|pick color|undertone)',
            r'(ai xem|who watch|view|lÆ°á»£t xem|viewer|follower)',
            r'(unboxing|má»Ÿ há»™p|first impression|áº¥n tÆ°á»£ng Ä‘áº§u)',
            r'(tutorial|hÆ°á»›ng dáº«n|how to|lÃ m tháº¿ nÃ o|guide)',
            r'(demo|demonstration|thá»­|test|sample|máº«u thá»­)'
        ]
    
    def _is_review_content(self, text: str) -> Tuple[bool, str]:
        """
        Determine if text is a genuine review or just a non-review comment
        Returns: (is_review, reason_if_not)
        """
        if not text or not text.strip():
            return False, "empty_text"
        
        text_lower = text.lower()
        
        # Check for spam patterns first
        for pattern in self.non_review_patterns['spam_patterns']:
            if re.search(pattern, text_lower, re.IGNORECASE):
                return False, "spam_content"
        
        # Check for greeting-only comments
        for pattern in self.non_review_patterns['greeting_only']:
            if re.search(pattern, text_lower, re.IGNORECASE):
                return False, "greeting_only"
        
        # Check for short reactions
        for pattern in self.non_review_patterns['short_reactions']:
            if re.search(pattern, text_lower, re.IGNORECASE):
                return False, "short_reaction"
        
        # Check for question-seeking help
        for pattern in self.non_review_patterns['question_seeking_help']:
            if re.search(pattern, text_lower, re.IGNORECASE):
                return False, "help_seeking"
        
        # Check for off-topic comments
        for pattern in self.non_review_patterns['off_topic']:
            if re.search(pattern, text_lower, re.IGNORECASE):
                return False, "off_topic"
        
        # Check for emotional-only comments
        for pattern in self.non_review_patterns['emotional_only']:
            if re.search(pattern, text_lower, re.IGNORECASE):
                return False, "emotional_only"
        
        # Check for non-product content (videos, marketing, etc.)
        for pattern in self.non_product_content_patterns:
            if re.search(pattern, text_lower, re.IGNORECASE):
                return False, "non_product_content"
        
        # Count review indicators
        review_score = 0
        for pattern in self.review_indicators:
            if re.search(pattern, text_lower, re.IGNORECASE):
                review_score += 1
        
        # Count product experience indicators for stricter filtering
        product_experience_score = 0
        for pattern in self.product_experience_indicators:
            if re.search(pattern, text_lower, re.IGNORECASE):
                product_experience_score += 1
        
        # Check minimum length for potential reviews
        word_count = len(text.split())
        
        # Apply enhanced review scoring logic with product experience
        if word_count < 5:
            if review_score == 0 and product_experience_score == 0:
                return False, "too_short_no_indicators"
        elif word_count < 10:
            if review_score < 1 and product_experience_score == 0:
                return False, "short_insufficient_indicators"
        elif word_count >= 10:
            # For longer reviews, require at least one product experience indicator
            # This is the key enhancement to filter out video reviews and marketing content
            if product_experience_score == 0 and review_score < 2:
                return False, "insufficient_product_experience"
        
        return True, "valid_review"
    
    def _detect_spam(self, text: str) -> bool:
        """Enhanced spam detection"""
        if not text:
            return False
        
        text_lower = text.lower()
        
        # Check for excessive repetition
        words = text_lower.split()
        if len(words) > 3:
            unique_words = set(words)
            repetition_ratio = 1 - (len(unique_words) / len(words))
            if repetition_ratio > 0.7:  # More than 70% repetition
                return True
        
        # Check for excessive punctuation or caps
        if len(text) > 10:
            caps_ratio = sum(1 for c in text if c.isupper()) / len(text)
            punct_ratio = sum(1 for c in text if c in '!?.,;:') / len(text)
            if caps_ratio > 0.6 or punct_ratio > 0.3:
                return True
        
        # Check for promotional content
        promotional_patterns = [
            r'(follow|theo dÃµi|Ä‘Äƒng kÃ½|subscribe)',
            r'(sale|giáº£m giÃ¡|promotion|khuyáº¿n mÃ£i)',
            r'(free|miá»…n phÃ­|gift|quÃ  táº·ng)',
            r'(win|tháº¯ng|lucky|may máº¯n|trÃºng thÆ°á»Ÿng)'
        ]
        
        promotional_count = 0
        for pattern in promotional_patterns:
            if re.search(pattern, text_lower, re.IGNORECASE):
                promotional_count += 1
        
        return promotional_count >= 2
    
    def process_data(self, raw_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Process scraped data through the preprocessing pipeline"""
        self.stats['total_loaded'] = len(raw_data)
        
        processed_items = []
        
        for item in raw_data:
            try:
                # Extract content
                content = item.get('content', '')
                
                # Quality filtering
                if not self._is_quality_text(content):
                    self.stats['filtered_quality'] += 1
                    continue
                
                # Duplicate detection
                if self.config.enable_deduplication:
                    text_hash = self._calculate_text_hash(content)
                    if text_hash in self.seen_hashes:
                        self.stats['filtered_duplicates'] += 1
                        continue
                    self.seen_hashes.add(text_hash)
                
                # Normalize content
                normalized_content = self._normalize_text(content)
                
                # Detect language
                language = self._detect_language(normalized_content)
                
                # Review filtering
                is_review, _ = self._is_review_content(normalized_content)
                if not is_review:
                    self.stats['filtered_non_review'] += 1
                    continue
                
                # Create processed item
                processed_item = {
                    'id': item.get('id', ''),
                    'content': normalized_content,
                    'source': item.get('source', 'unknown'),
                    'language': language,
                    'created_at': item.get('created_at', ''),
                    'metadata': item
                }
                
                processed_items.append(processed_item)
                self.stats['processed_successfully'] += 1
                
                # Update statistics
                source = item.get('source', 'unknown')
                self.stats['source_distribution'][source] = self.stats['source_distribution'].get(source, 0) + 1
                self.stats['language_distribution'][language] = self.stats['language_distribution'].get(language, 0) + 1
                
            except Exception as e:
                logger.error(f"Error processing item: {e}")
                continue
        
        return {
            'processed_data': processed_items,
            'stats': self.stats,
            'config': {
                'min_text_length': self.config.min_text_length,
                'max_text_length': self.config.max_text_length,
                'enable_deduplication': self.config.enable_deduplication,
                'target_language': self.config.target_language
            }
        }
